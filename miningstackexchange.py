from bs4 import BeautifulSoupimport requestsimport csvfrom urllib.request import urlopendef extractAnswerData(bitcoin_url):# =============================================================================# #   This function takes in a bitcoin stack exchange URL. #   	- The URL is for ONE question#   After the URL is taken, the function extracts the answer data#   	- the answer IDs, answer scores, author names, author reputations#     		the number of comments, and whether the answer is accepted# 	Finally this information is written into a .csv file# # =============================================================================    response = requests.get(bitcoin_url)  with urlopen (bitcoin_url) as url:      soup = BeautifulSoup(url, "lxml")      # to check if question is edited      edited_check = soup.find('div', {'class': 'user-action-time'}).text      if "edited" in edited_check:          edited_flag = True      else:          edited_flag = False   #   print(edited_flag)      # fields is the column names in the csv      fields = ["Answer_ID", "Answer_Score", "Author_Name", "Author_Rep",             "Number_Of_Comments", "Answer_Accepted"]      # opens the csv for writing      with open("bitcoin-answers.csv",'w') as csvfile:          csvwriter = csv.writer(csvfile)          csvwriter.writerow(fields)          # if the answer is accepted          for accepted_answer in soup.findAll('div', {'class': 'answer js-answer accepted-answer js-accepted-answer'}):              row = []              # gets answer id              ans_id = accepted_answer['data-answerid']            #  print("answer id: " + ans_id)              row.append(ans_id)              # gets answer score              ans_score = accepted_answer['data-score']           #   print("answer score: " + ans_score)              row.append(ans_score)              # gets author name              ans_author_name = accepted_answer.find('span', attrs={'itemprop': 'name'}).text           #   print("answer author: " + ans_author_name)              row.append(ans_author_name)              # gets author rep              ans_rep = accepted_answer.find('span', attrs={'class': 'reputation-score'}).text            #  print(ans_rep)              row.append(ans_rep)              # gets comment count, then checks if answer has no comments              ans_com_count = accepted_answer.find('span', attrs={'itemprop' :'commentCount'}).text              if not ans_com_count:                  ans_com_count = "0"           #   print("answer comment count: " + ans_com_count)              row.append(ans_com_count)                          accepted = True              row.append(accepted)              # writes to csv              csvwriter.writerow(row)          # else the answer was not accepted           for answer in soup.findAll('div', {'class': 'answer js-answer'}):              row = []              ans_id = answer['data-answerid']             # print("answer id: " + ans_id)              row.append(ans_id)              ans_score = answer['data-score']             # print("answer score: " + ans_score)              row.append(ans_score)              ans_author_name = answer.find('span', attrs={'itemprop': 'name'}).text             # print("answer author: " + ans_author_name)              row.append(ans_author_name)              ans_rep = answer.find('span', attrs={'class': 'reputation-score'}).text            #  print(ans_rep)              row.append(ans_rep)              ans_com_count = answer.find('span', attrs={'itemprop' :'commentCount'}).text              if not ans_com_count:                  ans_com_count = "0"            #  print("answer comment count: " + ans_com_count)              row.append(ans_com_count)              accepted = False            #  print(accepted)              row.append(accepted)              csvwriter.writerow(row)              #  NOTE: # each url should be one question. # this function should be ran in a for loop# EXAMPLE:# list_of_all_URLS = [SOME SHIT]# for url in list_of_all_URLS:#		extractAnswerData(url)                            